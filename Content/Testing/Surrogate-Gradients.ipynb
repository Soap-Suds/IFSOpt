{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25540ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../Sinkhorn/src')\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from ifst import Grid, LinearProblem, Sinkhorn\n",
    "import functools\n",
    "\n",
    "# Set random seed\n",
    "key = jax.random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mo4nq0as0ei",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brenier potential computed!\n",
      "Shape: (1048576,)\n",
      "Converged: True\n",
      "Sinkhorn distance: 0.332439\n"
     ]
    }
   ],
   "source": [
    "# Create two 1024x1024 images\n",
    "size = 1024\n",
    "\n",
    "# Create coordinate grid\n",
    "x = jnp.linspace(0, 1, size)\n",
    "y = jnp.linspace(0, 1, size)\n",
    "X, Y = jnp.meshgrid(x, y)\n",
    "\n",
    "# Image 1: Gaussian centered at (0.3, 0.3)\n",
    "image1 = jnp.exp(-100 * ((X - 0.3)**2 + (Y - 0.3)**2))\n",
    "\n",
    "# Image 2: Gaussian centered at (0.7, 0.7)\n",
    "image2 = jnp.exp(-100 * ((X - 0.7)**2 + (Y - 0.7)**2))\n",
    "\n",
    "# Normalize to probability distributions\n",
    "image1 = image1 / jnp.sum(image1)\n",
    "image2 = image2 / jnp.sum(image2)\n",
    "\n",
    "# Flatten the images\n",
    "a = image1.ravel()\n",
    "b = image2.ravel()\n",
    "\n",
    "# Create Grid geometry\n",
    "grid_geom = Grid(\n",
    "    grid_size=(size, size),\n",
    "    epsilon=0.01,  # Regularization parameter\n",
    ")\n",
    "\n",
    "# Create the linear OT problem\n",
    "ot_problem = LinearProblem(\n",
    "    geom=grid_geom,\n",
    "    a=a,\n",
    "    b=b\n",
    ")\n",
    "\n",
    "# Create and run the Sinkhorn solver\n",
    "solver = Sinkhorn(\n",
    "    threshold=1e-3,\n",
    "    max_iterations=1000,\n",
    "    lse_mode=True  # Use log-sum-exp mode for stability\n",
    ")\n",
    "\n",
    "# Solve the problem\n",
    "output = solver(ot_problem)\n",
    "\n",
    "# Extract the Brenier potential (dual potential f)\n",
    "brenier_potential = output.f\n",
    "\n",
    "print(f\"Brenier potential computed!\")\n",
    "print(f\"Shape: {brenier_potential.shape}\")\n",
    "print(f\"Converged: {output.converged}\")\n",
    "print(f\"Sinkhorn distance: {output.reg_ot_cost:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "194028f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Define Your Operator (Modified) ---\n",
    "# CHANGED: Now accepts side_length as an argument.\n",
    "def T_star_operator(potential: jax.Array, side_length: int) -> jax.Array:\n",
    "    \"\"\"\n",
    "    Applies the T*_{F,p} operator to a potential field.\n",
    "    \"\"\"\n",
    "    # CHANGED: The calculation is removed from here.\n",
    "    potential_2d = potential.reshape((side_length, side_length))\n",
    "    \n",
    "    shifted_potential = jnp.roll(potential_2d, shift=1, axis=0)\n",
    "    operated_potential_2d = (potential_2d + shifted_potential) * 0.45\n",
    "    \n",
    "    return operated_potential_2d.flatten()\n",
    "\n",
    "\n",
    "# --- Step 2: Define the JIT-Compiled Computation Function (Modified) ---\n",
    "# CHANGED: Added side_length to the decorator and function signature.\n",
    "@functools.partial(jax.jit, static_argnames=['num_iterations', 'side_length'])\n",
    "def compute_auxiliary_potential(phi_F: jax.Array, num_iterations: int, side_length: int) -> jax.Array:\n",
    "    \"\"\"\n",
    "    Computes the auxiliary potential by summing the truncated power series.\n",
    "    \"\"\"\n",
    "    psi_F = jnp.zeros_like(phi_F)\n",
    "    current_term = phi_F\n",
    "    \n",
    "    for _ in range(num_iterations):\n",
    "        psi_F += current_term\n",
    "        # CHANGED: Pass side_length to the operator.\n",
    "        current_term = T_star_operator(current_term, side_length=side_length)\n",
    "        \n",
    "    return psi_F\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=['side_length'])\n",
    "def compute_gradient_field(potential_flat: jax.Array, side_length: int) -> jax.Array:\n",
    "    \"\"\"\n",
    "    Computes the gradient of a scalar field.\n",
    "\n",
    "    Args:\n",
    "        potential_flat: A flattened 1D array representing the scalar field.\n",
    "        side_length: The size of one side of the 2D grid.\n",
    "\n",
    "    Returns:\n",
    "        A JAX array of shape (2, side_length, side_length) representing the\n",
    "        gradient vector field, where [0,:,:] is the Y-derivative and\n",
    "        [1,:,:] is the X-derivative.\n",
    "    \"\"\"\n",
    "    # Step 1: Reshape the potential from flat to a 2D grid\n",
    "    potential_2d = potential_flat.reshape((side_length, side_length))\n",
    "\n",
    "    # Step 2: Compute the gradient using the built-in JAX function\n",
    "    # This returns a list of 2 arrays: [gradient_along_axis_0, gradient_along_axis_1]\n",
    "    # For a 2D image/grid, axis 0 is the Y-direction and axis 1 is the X-direction.\n",
    "    grad_y, grad_x = jnp.gradient(potential_2d)\n",
    "\n",
    "    # Step 3: Stack the two components into a single vector field array\n",
    "    # The shape will be (2, 1024, 1024)\n",
    "    gradient_field = jnp.stack([grad_y, grad_x], axis=0)\n",
    "    \n",
    "    return gradient_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edcb9511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX is running on: NVIDIA GeForce RTX 2070 SUPER\n",
      "Calculated side_length: 1024\n",
      "\n",
      "Compiling and running the function...\n",
      "Computation complete.\n",
      "Shape of output potential: (1048576,)\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3: Example Usage (Modified) ---\n",
    "\n",
    "print(f\"JAX is running on: {jax.devices()[0].device_kind}\")\n",
    "\n",
    "grid_size = 1024\n",
    "key = jax.random.PRNGKey(0)\n",
    "phi_F = jax.random.normal(key, (grid_size * grid_size,))\n",
    "\n",
    "K = 20 \n",
    "\n",
    "# CHANGED: Calculate side_length ONCE, here in plain Python.\n",
    "# int() is fine here because we are not inside a JIT context.\n",
    "side_length = int(phi_F.shape[0]**0.5)\n",
    "print(f\"Calculated side_length: {side_length}\")\n",
    "\n",
    "print(\"\\nCompiling and running the function...\")\n",
    "# CHANGED: Pass the pre-calculated side_length to the function.\n",
    "auxiliary_potential = compute_auxiliary_potential(phi_F, num_iterations=K, side_length=side_length)\n",
    "\n",
    "auxiliary_potential.block_until_ready()\n",
    "print(\"Computation complete.\")\n",
    "print(f\"Shape of output potential: {auxiliary_potential.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dda8a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling and running the gradient computation...\n",
      "Computation complete.\n",
      "Shape of the input potential (flat): (1048576,)\n",
      "Shape of the output gradient field: (2, 1024, 1024)\n",
      "\n",
      "The output shape (2, 1024, 1024) represents:\n",
      "  - Gradient Y-component shape: (1024, 1024)\n",
      "  - Gradient X-component shape: (1024, 1024)\n"
     ]
    }
   ],
   "source": [
    "# --- Example Usage ---\n",
    "\n",
    "# Compute the gradient vector field\n",
    "print(\"Compiling and running the gradient computation...\")\n",
    "grad_vector_field = compute_gradient_field(auxiliary_potential, side_length=grid_size)\n",
    "\n",
    "# Block until the computation is finished on the GPU to see the result\n",
    "grad_vector_field.block_until_ready()\n",
    "\n",
    "print(\"Computation complete.\")\n",
    "print(f\"Shape of the input potential (flat): {auxiliary_potential.shape}\")\n",
    "print(f\"Shape of the output gradient field: {grad_vector_field.shape}\")\n",
    "print(\"\\nThe output shape (2, 1024, 1024) represents:\")\n",
    "print(f\"  - Gradient Y-component shape: {grad_vector_field[0].shape}\")\n",
    "print(f\"  - Gradient X-component shape: {grad_vector_field[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "929d09cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(jax.jit, static_argnames=['d'])\n",
    "def _compute_transformed_coords(f: jnp.ndarray, base_grid: jnp.ndarray, d: int) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Compute transformed coordinates for pulling back a vector field through f.\n",
    "    \n",
    "    Args:\n",
    "        f: Transformation matrix (3, 3)\n",
    "        base_grid: Base grid in homogeneous coordinates (d*d, 3)\n",
    "        d: Grid dimension\n",
    "        \n",
    "    Returns:\n",
    "        Pixel coordinates (2, d, d) for map_coordinates\n",
    "    \"\"\"\n",
    "    # Apply transformation f to grid points\n",
    "    transformed = f @ base_grid.T  # (3, d*d)\n",
    "    \n",
    "    # De-homogenize\n",
    "    x_norm = transformed[0] / transformed[2]  # (d*d,)\n",
    "    y_norm = transformed[1] / transformed[2]  # (d*d,)\n",
    "    \n",
    "    # Convert to pixel indices [0, d-1]\n",
    "    x_pixel = x_norm * (d - 1.0)\n",
    "    y_pixel = y_norm * (d - 1.0)\n",
    "    \n",
    "    # Stack as (y, x) for map_coordinates\n",
    "    coords = jnp.stack([\n",
    "        y_pixel.reshape(d, d),\n",
    "        x_pixel.reshape(d, d)\n",
    "    ], axis=0)\n",
    "    \n",
    "    return coords\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=['d'])\n",
    "def _pullback_vector_field(T: jnp.ndarray, f: jnp.ndarray, base_grid: jnp.ndarray, d: int) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Pull back vector field T by transformation f.\n",
    "    \n",
    "    Computes T ∘ f, i.e., T(f(x)) at each grid point x.\n",
    "    \n",
    "    Args:\n",
    "        T: Vector field (2, d, d) where T[0] is y-component, T[1] is x-component\n",
    "        f: Transformation matrix (3, 3)\n",
    "        base_grid: Base grid in homogeneous coordinates (d*d, 3)\n",
    "        d: Grid dimension\n",
    "        \n",
    "    Returns:\n",
    "        Pulled-back vector field (2, d, d)\n",
    "    \"\"\"\n",
    "    # Get transformed coordinates\n",
    "    coords = _compute_transformed_coords(f, base_grid, d)\n",
    "    \n",
    "    # Pull back each component of the vector field\n",
    "    T_y_pulled = jax.scipy.ndimage.map_coordinates(\n",
    "        T[0], coords, order=1, mode='constant', cval=0.0\n",
    "    )\n",
    "    T_x_pulled = jax.scipy.ndimage.map_coordinates(\n",
    "        T[1], coords, order=1, mode='constant', cval=0.0\n",
    "    )\n",
    "    \n",
    "    # Stack components\n",
    "    T_pulled = jnp.stack([T_y_pulled, T_x_pulled], axis=0)\n",
    "    \n",
    "    return T_pulled\n",
    "\n",
    "\n",
    "def IFSgradient(F, p, T, rho_F):\n",
    "    \"\"\"\n",
    "    Compute gradients for IFS optimization.\n",
    "    \n",
    "    For each transformation f_i in F, computes:\n",
    "        grad_i = p_i * rho_F(x) * T(f_i(x))\n",
    "    \n",
    "    where T(f_i(x)) is the pull-back of the vector field T by f_i.\n",
    "    \n",
    "    Args:\n",
    "        F: List of n transformation matrices (each 3x3 JAX array)\n",
    "        p: Probability vector (n,) JAX array\n",
    "        T: Gradient vector field (2, d, d) where T[0] is y-component, T[1] is x-component\n",
    "        rho_F: Fixed measure (d, d) JAX array\n",
    "        \n",
    "    Returns:\n",
    "        Fgrads: List of n gradient vector fields, each (2, d, d)\n",
    "        pgrad: Gradient w.r.t. probabilities (n,) - to be implemented\n",
    "    \"\"\"\n",
    "    n = len(F)\n",
    "    d = rho_F.shape[0]\n",
    "    \n",
    "    # Create base grid (same as in FixedMeasureSolver)\n",
    "    y_coords = jnp.linspace(0.0, 1.0, d, dtype=jnp.float32)\n",
    "    x_coords = jnp.linspace(0.0, 1.0, d, dtype=jnp.float32)\n",
    "    grid_y, grid_x = jnp.meshgrid(y_coords, x_coords, indexing='ij')\n",
    "    \n",
    "    base_grid = jnp.stack([\n",
    "        grid_x.ravel(),\n",
    "        grid_y.ravel(),\n",
    "        jnp.ones(d * d, dtype=jnp.float32)\n",
    "    ], axis=1)  # (d*d, 3)\n",
    "    \n",
    "    # Compute gradients for each transformation\n",
    "    Fgrads = []\n",
    "    for i in range(n):\n",
    "        f_i = F[i]\n",
    "        p_i = p[i]\n",
    "        \n",
    "        # Pull back T by f_i: T(f_i(x))\n",
    "        T_pulled = _pullback_vector_field(T, f_i, base_grid, d)\n",
    "        \n",
    "        # Multiply by p_i and rho_F(x): p_i * rho_F(x) * T(f_i(x))\n",
    "        # Broadcasting: (2, d, d) * scalar * (d, d) -> (2, d, d)\n",
    "        grad_i = p_i * rho_F[None, :, :] * T_pulled\n",
    "        \n",
    "        Fgrads.append(grad_i)\n",
    "    \n",
    "    # TODO: Compute gradient w.r.t. p (probability vector)\n",
    "    # For now, placeholder\n",
    "    pgrad = jnp.zeros_like(p)\n",
    "    \n",
    "    return Fgrads, pgrad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mq0m30f5g39",
   "metadata": {},
   "source": [
    "## IFS Gradient Computation\n",
    "\n",
    "This cell implements the gradient computation for IFS optimization using surrogate gradients.\n",
    "\n",
    "### Mathematical Background\n",
    "\n",
    "For an IFS defined by transformations $f_1, \\ldots, f_n$ with probabilities $p_1, \\ldots, p_n$ and fixed measure $\\rho_F$, the gradient with respect to each transformation $f_i$ is:\n",
    "\n",
    "$$\\text{grad}_i = p_i \\cdot \\rho_F(x) \\cdot T(f_i(x))$$\n",
    "\n",
    "where:\n",
    "- $T$ is a vector field (the gradient of the Brenier potential)\n",
    "- $T(f_i(x))$ is the pull-back of $T$ by the transformation $f_i$\n",
    "- $\\rho_F(x)$ is the fixed measure\n",
    "- $p_i$ is the probability weight\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "The key computational challenge is computing $T(f_i(x))$ efficiently:\n",
    "\n",
    "1. **Grid transformation**: Since $f_i$ is affine, we can compute where each grid point maps to\n",
    "2. **Interpolation**: Use `map_coordinates` to pull back the vector field $T$\n",
    "3. **Pointwise multiplication**: Multiply by $p_i$ and $\\rho_F(x)$\n",
    "\n",
    "The implementation reuses the grid transformation logic from the `ifs_solver` package for efficiency and consistency.\n",
    "\n",
    "### Performance Notes\n",
    "\n",
    "- The pull-back operation uses bilinear interpolation (order=1)\n",
    "- Each component of the vector field is pulled back separately\n",
    "- The base grid is created once and reused for all transformations\n",
    "- The functions are JIT-compiled for optimal performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ovoptfj8px",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing IFSgradient function...\n",
      "======================================================================\n",
      "T_test shape: (2, 128, 128)\n",
      "rho_F_test shape: (128, 128)\n",
      "\n",
      "Computing IFS gradients...\n",
      "\n",
      "Results:\n",
      "Number of F gradients: 3\n",
      "Shape of each F gradient: (2, 128, 128)\n",
      "pgrad shape: (3,)\n",
      "\n",
      "Gradient statistics:\n",
      "  F[0] gradient: mean=-6.546317e-07, max=1.710885e-04\n",
      "  F[1] gradient: mean=-5.915949e-07, max=1.542660e-04\n",
      "  F[2] gradient: mean=-4.123000e-07, max=2.041800e-04\n",
      "\n",
      "✓ IFSgradient function works!\n"
     ]
    }
   ],
   "source": [
    "# Test IFSgradient function\n",
    "print(\"Testing IFSgradient function...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create test IFS (Sierpinski triangle)\n",
    "F_test = [\n",
    "    jnp.array([\n",
    "        [0.5, 0.0, 0.0],\n",
    "        [0.0, 0.5, 0.0],\n",
    "        [0.0, 0.0, 1.0]\n",
    "    ], dtype=jnp.float32),\n",
    "    jnp.array([\n",
    "        [0.5, 0.0, 0.5],\n",
    "        [0.0, 0.5, 0.0],\n",
    "        [0.0, 0.0, 1.0]\n",
    "    ], dtype=jnp.float32),\n",
    "    jnp.array([\n",
    "        [0.5, 0.0, 0.0],\n",
    "        [0.0, 0.5, 0.5],\n",
    "        [0.0, 0.0, 1.0]\n",
    "    ], dtype=jnp.float32)\n",
    "]\n",
    "\n",
    "p_test = jnp.array([1/3, 1/3, 1/3], dtype=jnp.float32)\n",
    "\n",
    "# Create test fixed measure (uniform for simplicity)\n",
    "d_test = 128\n",
    "rho_F_test = jnp.ones((d_test, d_test), dtype=jnp.float32) / (d_test * d_test)\n",
    "\n",
    "# Create test gradient field (using the one we computed earlier, downsampled)\n",
    "# For testing, we'll create a simple gradient field\n",
    "T_test = grad_vector_field[:, ::8, ::8]  # Downsample from 1024 to 128\n",
    "print(f\"T_test shape: {T_test.shape}\")\n",
    "print(f\"rho_F_test shape: {rho_F_test.shape}\")\n",
    "\n",
    "# Compute gradients\n",
    "print(\"\\nComputing IFS gradients...\")\n",
    "Fgrads, pgrad = IFSgradient(F_test, p_test, T_test, rho_F_test)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"Number of F gradients: {len(Fgrads)}\")\n",
    "print(f\"Shape of each F gradient: {Fgrads[0].shape}\")\n",
    "print(f\"pgrad shape: {pgrad.shape}\")\n",
    "\n",
    "# Check properties\n",
    "print(f\"\\nGradient statistics:\")\n",
    "for i, grad in enumerate(Fgrads):\n",
    "    print(f\"  F[{i}] gradient: mean={jnp.mean(grad):.6e}, max={jnp.max(jnp.abs(grad)):.6e}\")\n",
    "\n",
    "print(\"\\n✓ IFSgradient function works!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "vd8ghweu89r",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing optimized IFSgradient...\n",
      "======================================================================\n",
      "F_test_stacked shape: (3, 3, 3)\n",
      "\n",
      "Optimized results:\n",
      "Fgrads shape: (3, 2, 128, 128)\n",
      "pgrad shape: (3,)\n",
      "Time: 0.5309s\n",
      "\n",
      "Comparing with original implementation...\n",
      "Original time: 0.0059s\n",
      "Max difference F[0]: 0.00e+00\n",
      "Max difference F[1]: 0.00e+00\n",
      "Max difference F[2]: 0.00e+00\n",
      "\n",
      "(First call includes compilation overhead)\n",
      "\n",
      "✓ Optimized IFSgradient works!\n"
     ]
    }
   ],
   "source": [
    "# Optimized version using vmap for parallelization\n",
    "@functools.partial(jax.jit, static_argnames=['d'])\n",
    "def IFSgradient_optimized(F, p, T, rho_F, d):\n",
    "    \"\"\"\n",
    "    Optimized IFS gradient computation using vmap.\n",
    "    \n",
    "    Args:\n",
    "        F: Stacked transformation matrices (n, 3, 3)\n",
    "        p: Probability vector (n,)\n",
    "        T: Gradient vector field (2, d, d)\n",
    "        rho_F: Fixed measure (d, d)\n",
    "        d: Grid dimension (static argument for JIT)\n",
    "        \n",
    "    Returns:\n",
    "        Fgrads: Gradient vector fields (n, 2, d, d)\n",
    "        pgrad: Gradient w.r.t. probabilities (n,)\n",
    "    \"\"\"\n",
    "    # Create base grid once\n",
    "    y_coords = jnp.linspace(0.0, 1.0, d, dtype=jnp.float32)\n",
    "    x_coords = jnp.linspace(0.0, 1.0, d, dtype=jnp.float32)\n",
    "    grid_y, grid_x = jnp.meshgrid(y_coords, x_coords, indexing='ij')\n",
    "    \n",
    "    base_grid = jnp.stack([\n",
    "        grid_x.ravel(),\n",
    "        grid_y.ravel(),\n",
    "        jnp.ones(d * d, dtype=jnp.float32)\n",
    "    ], axis=1)  # (d*d, 3)\n",
    "    \n",
    "    # Vectorized pull-back across all transformations\n",
    "    # vmap over axis 0 of F\n",
    "    T_pulled_all = jax.vmap(\n",
    "        lambda f: _pullback_vector_field(T, f, base_grid, d)\n",
    "    )(F)  # (n, 2, d, d)\n",
    "    \n",
    "    # Compute gradients: p_i * rho_F * T_pulled_i\n",
    "    # Broadcasting: (n, 1, 1, 1) * (1, 1, d, d) * (n, 2, d, d)\n",
    "    Fgrads = p[:, None, None, None] * rho_F[None, None, :, :] * T_pulled_all\n",
    "    \n",
    "    # TODO: Gradient w.r.t. p\n",
    "    pgrad = jnp.zeros_like(p)\n",
    "    \n",
    "    return Fgrads, pgrad\n",
    "\n",
    "\n",
    "# Test the optimized version\n",
    "print(\"\\nTesting optimized IFSgradient...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Stack F for the optimized version\n",
    "F_test_stacked = jnp.stack(F_test, axis=0)  # (3, 3, 3)\n",
    "\n",
    "print(f\"F_test_stacked shape: {F_test_stacked.shape}\")\n",
    "\n",
    "# Compute gradients (optimized)\n",
    "import time\n",
    "start = time.perf_counter()\n",
    "Fgrads_opt, pgrad_opt = IFSgradient_optimized(F_test_stacked, p_test, T_test, rho_F_test, d=d_test)\n",
    "Fgrads_opt[0].block_until_ready()\n",
    "time_opt = time.perf_counter() - start\n",
    "\n",
    "print(f\"\\nOptimized results:\")\n",
    "print(f\"Fgrads shape: {Fgrads_opt.shape}\")\n",
    "print(f\"pgrad shape: {pgrad_opt.shape}\")\n",
    "print(f\"Time: {time_opt:.4f}s\")\n",
    "\n",
    "# Compare with original version\n",
    "print(f\"\\nComparing with original implementation...\")\n",
    "start = time.perf_counter()\n",
    "Fgrads_orig, pgrad_orig = IFSgradient(F_test, p_test, T_test, rho_F_test)\n",
    "Fgrads_orig[0].block_until_ready()\n",
    "time_orig = time.perf_counter() - start\n",
    "print(f\"Original time: {time_orig:.4f}s\")\n",
    "\n",
    "# Check if results match\n",
    "for i in range(len(F_test)):\n",
    "    diff = jnp.max(jnp.abs(Fgrads_opt[i] - Fgrads_orig[i]))\n",
    "    print(f\"Max difference F[{i}]: {diff:.2e}\")\n",
    "\n",
    "if time_orig > time_opt:\n",
    "    speedup = time_orig / time_opt\n",
    "    print(f\"\\n✓ Optimized version is {speedup:.2f}x faster!\")\n",
    "else:\n",
    "    print(f\"\\n(First call includes compilation overhead)\")\n",
    "    \n",
    "print(\"\\n✓ Optimized IFSgradient works!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bjn0ag4gcke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing compute_p_gradient...\n",
      "======================================================================\n",
      "psi_test shape: (128, 128)\n",
      "rho_F_test shape: (128, 128)\n",
      "F_test_stacked shape: (3, 3, 3)\n",
      "\n",
      "Results:\n",
      "pgrad shape: (3,)\n",
      "pgrad values: [-0.04286745 -0.02920848  0.05633599]\n",
      "\n",
      "Gradient statistics:\n",
      "  Mean: -5.246644e-03\n",
      "  Max: 5.633599e-02\n",
      "  Min: -4.286745e-02\n",
      "  Std: 4.390109e-02\n",
      "\n",
      "✓ compute_p_gradient works!\n"
     ]
    }
   ],
   "source": [
    "@functools.partial(jax.jit, static_argnames=['d'])\n",
    "def _pullback_scalar_field(psi: jnp.ndarray, f: jnp.ndarray, base_grid: jnp.ndarray, d: int) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Pull back scalar field psi by transformation f.\n",
    "    \n",
    "    Computes psi ∘ f, i.e., psi(f(x)) at each grid point x.\n",
    "    \n",
    "    Args:\n",
    "        psi: Scalar field (d, d)\n",
    "        f: Transformation matrix (3, 3)\n",
    "        base_grid: Base grid in homogeneous coordinates (d*d, 3)\n",
    "        d: Grid dimension\n",
    "        \n",
    "    Returns:\n",
    "        Pulled-back scalar field (d, d)\n",
    "    \"\"\"\n",
    "    # Get transformed coordinates\n",
    "    coords = _compute_transformed_coords(f, base_grid, d)\n",
    "    \n",
    "    # Pull back the scalar field using bilinear interpolation\n",
    "    psi_pulled = jax.scipy.ndimage.map_coordinates(\n",
    "        psi, coords, order=1, mode='constant', cval=0.0\n",
    "    )\n",
    "    \n",
    "    return psi_pulled\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=['d'])\n",
    "def compute_p_gradient(F, rho_F, psi, d):\n",
    "    \"\"\"\n",
    "    Compute gradient with respect to probability vector p.\n",
    "    \n",
    "    For each p_i, the gradient is:\n",
    "        grad_p[i] = ∫_X rho_F(x) * psi(f_i(x)) dx\n",
    "    \n",
    "    where the integral is a sum over the discrete grid X.\n",
    "    \n",
    "    Args:\n",
    "        F: Stacked transformation matrices (n, 3, 3)\n",
    "        rho_F: Fixed measure (d, d)\n",
    "        psi: Auxiliary potential (d, d) - scalar field\n",
    "        d: Grid dimension (static argument)\n",
    "        \n",
    "    Returns:\n",
    "        pgrad: Gradient w.r.t. probabilities (n,)\n",
    "    \"\"\"\n",
    "    # Create base grid once\n",
    "    y_coords = jnp.linspace(0.0, 1.0, d, dtype=jnp.float32)\n",
    "    x_coords = jnp.linspace(0.0, 1.0, d, dtype=jnp.float32)\n",
    "    grid_y, grid_x = jnp.meshgrid(y_coords, x_coords, indexing='ij')\n",
    "    \n",
    "    base_grid = jnp.stack([\n",
    "        grid_x.ravel(),\n",
    "        grid_y.ravel(),\n",
    "        jnp.ones(d * d, dtype=jnp.float32)\n",
    "    ], axis=1)  # (d*d, 3)\n",
    "    \n",
    "    # Vectorized pull-back of psi across all transformations\n",
    "    # vmap over axis 0 of F\n",
    "    psi_pulled_all = jax.vmap(\n",
    "        lambda f: _pullback_scalar_field(psi, f, base_grid, d)\n",
    "    )(F)  # (n, d, d)\n",
    "    \n",
    "    # Compute integral: sum over grid of rho_F(x) * psi(f_i(x))\n",
    "    # Broadcasting: (1, d, d) * (n, d, d) -> (n, d, d)\n",
    "    integrand = rho_F[None, :, :] * psi_pulled_all  # (n, d, d)\n",
    "    \n",
    "    # Sum over spatial dimensions to get gradient for each p_i\n",
    "    pgrad = jnp.sum(integrand, axis=(1, 2))  # (n,)\n",
    "    \n",
    "    return pgrad\n",
    "\n",
    "\n",
    "# Update the optimized IFSgradient to not compute pgrad\n",
    "@functools.partial(jax.jit, static_argnames=['d'])\n",
    "def IFSgradient_F_only(F, p, T, rho_F, d):\n",
    "    \"\"\"\n",
    "    Compute gradients with respect to F only (for surrogate gradient method).\n",
    "    \n",
    "    Note: Gradient w.r.t. p is computed separately using compute_p_gradient()\n",
    "    since it requires the auxiliary potential psi, not the vector field T.\n",
    "    \n",
    "    Args:\n",
    "        F: Stacked transformation matrices (n, 3, 3)\n",
    "        p: Probability vector (n,)\n",
    "        T: Gradient vector field (2, d, d)\n",
    "        rho_F: Fixed measure (d, d)\n",
    "        d: Grid dimension (static argument for JIT)\n",
    "        \n",
    "    Returns:\n",
    "        Fgrads: Gradient vector fields (n, 2, d, d)\n",
    "    \"\"\"\n",
    "    # Create base grid once\n",
    "    y_coords = jnp.linspace(0.0, 1.0, d, dtype=jnp.float32)\n",
    "    x_coords = jnp.linspace(0.0, 1.0, d, dtype=jnp.float32)\n",
    "    grid_y, grid_x = jnp.meshgrid(y_coords, x_coords, indexing='ij')\n",
    "    \n",
    "    base_grid = jnp.stack([\n",
    "        grid_x.ravel(),\n",
    "        grid_y.ravel(),\n",
    "        jnp.ones(d * d, dtype=jnp.float32)\n",
    "    ], axis=1)  # (d*d, 3)\n",
    "    \n",
    "    # Vectorized pull-back across all transformations\n",
    "    T_pulled_all = jax.vmap(\n",
    "        lambda f: _pullback_vector_field(T, f, base_grid, d)\n",
    "    )(F)  # (n, 2, d, d)\n",
    "    \n",
    "    # Compute gradients: p_i * rho_F * T_pulled_i\n",
    "    Fgrads = p[:, None, None, None] * rho_F[None, None, :, :] * T_pulled_all\n",
    "    \n",
    "    return Fgrads\n",
    "\n",
    "\n",
    "# Test the p gradient computation\n",
    "print(\"\\nTesting compute_p_gradient...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create a test auxiliary potential (using the one computed earlier, downsampled)\n",
    "psi_test = auxiliary_potential.reshape(grid_size, grid_size)[::8, ::8]  # Downsample to 128x128\n",
    "print(f\"psi_test shape: {psi_test.shape}\")\n",
    "print(f\"rho_F_test shape: {rho_F_test.shape}\")\n",
    "print(f\"F_test_stacked shape: {F_test_stacked.shape}\")\n",
    "\n",
    "# Compute p gradient\n",
    "pgrad_test = compute_p_gradient(F_test_stacked, rho_F_test, psi_test, d=d_test)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"pgrad shape: {pgrad_test.shape}\")\n",
    "print(f\"pgrad values: {pgrad_test}\")\n",
    "\n",
    "print(f\"\\nGradient statistics:\")\n",
    "print(f\"  Mean: {jnp.mean(pgrad_test):.6e}\")\n",
    "print(f\"  Max: {jnp.max(pgrad_test):.6e}\")\n",
    "print(f\"  Min: {jnp.min(pgrad_test):.6e}\")\n",
    "print(f\"  Std: {jnp.std(pgrad_test):.6e}\")\n",
    "\n",
    "print(\"\\n✓ compute_p_gradient works!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "uz4kp5u62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPLETE IFS GRADIENT COMPUTATION WORKFLOW\n",
      "================================================================================\n",
      "\n",
      "[Step 1] Setting up IFS...\n",
      "  IFS transformations: 3 maps\n",
      "  Probabilities: [0.33333334 0.33333334 0.33333334]\n",
      "\n",
      "[Step 2] Computing fixed measure ρ_F...\n",
      "  Fixed measure shape: (128, 128)\n",
      "  Total mass: 1.000000\n",
      "\n",
      "[Step 3] Preparing gradient field T...\n",
      "  Gradient field shape: (2, 128, 128)\n",
      "  Components: T[0] (y-gradient), T[1] (x-gradient)\n",
      "\n",
      "[Step 4] Preparing auxiliary potential ψ...\n",
      "  Auxiliary potential shape: (128, 128)\n",
      "  Value range: [-12.5424, 13.8567]\n",
      "\n",
      "[Step 5] Computing gradients w.r.t. F...\n",
      "  F gradients shape: (3, 2, 128, 128)\n",
      "  Interpretation: 3 transformations, each with 2D vector field\n",
      "    F[0] gradient L2 norm: 5.1133e-03\n",
      "    F[1] gradient L2 norm: 5.0517e-03\n",
      "    F[2] gradient L2 norm: 5.1160e-03\n",
      "\n",
      "[Step 6] Computing gradients w.r.t. p...\n",
      "  p gradient shape: (3,)\n",
      "  p gradient values: [-0.04286745 -0.02920848  0.05633599]\n",
      "  Gradient magnitude: 7.6580e-02\n",
      "\n",
      "================================================================================\n",
      "GRADIENT COMPUTATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Inputs:\n",
      "  - IFS transformations F:     (3, 3, 3)\n",
      "  - Probabilities p:            (3,)\n",
      "  - Fixed measure ρ_F:          (128, 128)\n",
      "  - Gradient field T:           (2, 128, 128)\n",
      "  - Auxiliary potential ψ:      (128, 128)\n",
      "\n",
      "Outputs:\n",
      "  - Gradients w.r.t. F:         (3, 2, 128, 128)  (n vector fields)\n",
      "  - Gradient w.r.t. p:          (3,)   (n scalars)\n",
      "\n",
      "Total gradients computed:       3 (for F) + 3 (for p)\n",
      "                              = 6 total gradient components\n",
      "\n",
      "✓ All gradients computed successfully!\n",
      "✓ Ready for optimization loop!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# COMPLETE WORKFLOW EXAMPLE: Computing all gradients for IFS optimization\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPLETE IFS GRADIENT COMPUTATION WORKFLOW\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 1: Set up IFS parameters\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n[Step 1] Setting up IFS...\")\n",
    "\n",
    "# Example: Sierpinski triangle IFS\n",
    "F_example = jnp.stack([\n",
    "    jnp.array([[0.5, 0.0, 0.0],\n",
    "               [0.0, 0.5, 0.0],\n",
    "               [0.0, 0.0, 1.0]], dtype=jnp.float32),\n",
    "    jnp.array([[0.5, 0.0, 0.5],\n",
    "               [0.0, 0.5, 0.0],\n",
    "               [0.0, 0.0, 1.0]], dtype=jnp.float32),\n",
    "    jnp.array([[0.5, 0.0, 0.0],\n",
    "               [0.0, 0.5, 0.5],\n",
    "               [0.0, 0.0, 1.0]], dtype=jnp.float32)\n",
    "], axis=0)  # Shape: (3, 3, 3)\n",
    "\n",
    "p_example = jnp.array([1/3, 1/3, 1/3], dtype=jnp.float32)\n",
    "\n",
    "print(f\"  IFS transformations: {F_example.shape[0]} maps\")\n",
    "print(f\"  Probabilities: {p_example}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 2: Compute fixed measure ρ_F (using FixedMeasureSolver from ifs_solver)\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n[Step 2] Computing fixed measure ρ_F...\")\n",
    "\n",
    "# For this example, we'll use a uniform distribution\n",
    "# In practice, you'd use: solver.solve(F=F_example, p=p_example)\n",
    "d_example = 128\n",
    "rho_F_example = jnp.ones((d_example, d_example), dtype=jnp.float32) / (d_example**2)\n",
    "\n",
    "print(f\"  Fixed measure shape: {rho_F_example.shape}\")\n",
    "print(f\"  Total mass: {jnp.sum(rho_F_example):.6f}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 3: Prepare gradient field T (from Brenier potential)\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n[Step 3] Preparing gradient field T...\")\n",
    "\n",
    "# Downsample the gradient field we computed earlier\n",
    "T_example = grad_vector_field[:, ::8, ::8]  # From 1024 to 128\n",
    "\n",
    "print(f\"  Gradient field shape: {T_example.shape}\")\n",
    "print(f\"  Components: T[0] (y-gradient), T[1] (x-gradient)\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 4: Prepare auxiliary potential ψ\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n[Step 4] Preparing auxiliary potential ψ...\")\n",
    "\n",
    "# Downsample the auxiliary potential\n",
    "psi_example = auxiliary_potential.reshape(grid_size, grid_size)[::8, ::8]\n",
    "\n",
    "print(f\"  Auxiliary potential shape: {psi_example.shape}\")\n",
    "print(f\"  Value range: [{jnp.min(psi_example):.4f}, {jnp.max(psi_example):.4f}]\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 5: Compute gradient w.r.t. F\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n[Step 5] Computing gradients w.r.t. F...\")\n",
    "\n",
    "Fgrads_example = IFSgradient_F_only(F_example, p_example, T_example, rho_F_example, d=d_example)\n",
    "\n",
    "print(f\"  F gradients shape: {Fgrads_example.shape}\")\n",
    "print(f\"  Interpretation: {Fgrads_example.shape[0]} transformations, each with 2D vector field\")\n",
    "\n",
    "for i in range(Fgrads_example.shape[0]):\n",
    "    grad_norm = jnp.sqrt(jnp.sum(Fgrads_example[i]**2))\n",
    "    print(f\"    F[{i}] gradient L2 norm: {grad_norm:.4e}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 6: Compute gradient w.r.t. p\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n[Step 6] Computing gradients w.r.t. p...\")\n",
    "\n",
    "pgrad_example = compute_p_gradient(F_example, rho_F_example, psi_example, d=d_example)\n",
    "\n",
    "print(f\"  p gradient shape: {pgrad_example.shape}\")\n",
    "print(f\"  p gradient values: {pgrad_example}\")\n",
    "print(f\"  Gradient magnitude: {jnp.linalg.norm(pgrad_example):.4e}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Summary\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GRADIENT COMPUTATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\"\"\n",
    "Inputs:\n",
    "  - IFS transformations F:     {F_example.shape}\n",
    "  - Probabilities p:            {p_example.shape}\n",
    "  - Fixed measure ρ_F:          {rho_F_example.shape}\n",
    "  - Gradient field T:           {T_example.shape}\n",
    "  - Auxiliary potential ψ:      {psi_example.shape}\n",
    "\n",
    "Outputs:\n",
    "  - Gradients w.r.t. F:         {Fgrads_example.shape}  (n vector fields)\n",
    "  - Gradient w.r.t. p:          {pgrad_example.shape}   (n scalars)\n",
    "\n",
    "Total gradients computed:       {Fgrads_example.shape[0]} (for F) + {pgrad_example.shape[0]} (for p)\n",
    "                              = {Fgrads_example.shape[0] + pgrad_example.shape[0]} total gradient components\n",
    "\n",
    "✓ All gradients computed successfully!\n",
    "✓ Ready for optimization loop!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h25y6zj166w",
   "metadata": {},
   "source": [
    "## API Summary\n",
    "\n",
    "### Functions for IFS Gradient Computation\n",
    "\n",
    "#### 1. **IFSgradient_F_only** - Compute gradients w.r.t. transformations F\n",
    "\n",
    "```python\n",
    "Fgrads = IFSgradient_F_only(F, p, T, rho_F, d)\n",
    "```\n",
    "\n",
    "**Inputs:**\n",
    "- `F`: Stacked transformation matrices `(n, 3, 3)`\n",
    "- `p`: Probability vector `(n,)`\n",
    "- `T`: Gradient vector field `(2, d, d)` from Brenier potential\n",
    "- `rho_F`: Fixed measure `(d, d)`\n",
    "- `d`: Grid dimension (static, for JIT)\n",
    "\n",
    "**Output:**\n",
    "- `Fgrads`: Gradient vector fields `(n, 2, d, d)`\n",
    "\n",
    "**Formula:** For each i: `Fgrads[i] = p[i] * rho_F(x) * T(f_i(x))`\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **compute_p_gradient** - Compute gradient w.r.t. probability vector p\n",
    "\n",
    "```python\n",
    "pgrad = compute_p_gradient(F, rho_F, psi, d)\n",
    "```\n",
    "\n",
    "**Inputs:**\n",
    "- `F`: Stacked transformation matrices `(n, 3, 3)`\n",
    "- `rho_F`: Fixed measure `(d, d)`\n",
    "- `psi`: Auxiliary potential `(d, d)` (scalar field)\n",
    "- `d`: Grid dimension (static, for JIT)\n",
    "\n",
    "**Output:**\n",
    "- `pgrad`: Gradient scalars `(n,)`\n",
    "\n",
    "**Formula:** For each i: `pgrad[i] = ∫_X rho_F(x) * psi(f_i(x)) dx`\n",
    "\n",
    "---\n",
    "\n",
    "### Helper Functions (Internal)\n",
    "\n",
    "- **`_compute_transformed_coords(f, base_grid, d)`** - Transform grid coordinates by affine map\n",
    "- **`_pullback_vector_field(T, f, base_grid, d)`** - Pull back vector field through transformation\n",
    "- **`_pullback_scalar_field(psi, f, base_grid, d)`** - Pull back scalar field through transformation\n",
    "\n",
    "All functions are JIT-compiled for performance.\n",
    "\n",
    "---\n",
    "\n",
    "### Usage in Optimization Loop\n",
    "\n",
    "```python\n",
    "# Setup (once)\n",
    "d = 128\n",
    "F = jnp.stack([f1, f2, f3], axis=0)  # (3, 3, 3)\n",
    "p = jnp.array([1/3, 1/3, 1/3])\n",
    "\n",
    "# In each iteration:\n",
    "# 1. Compute fixed measure\n",
    "rho_F = solver.solve(F=F, p=p)\n",
    "\n",
    "# 2. Compute OT and get potentials\n",
    "T = compute_gradient_field(brenier_potential, d)\n",
    "psi = auxiliary_potential.reshape(d, d)\n",
    "\n",
    "# 3. Compute gradients\n",
    "Fgrads = IFSgradient_F_only(F, p, T, rho_F, d)\n",
    "pgrad = compute_p_gradient(F, rho_F, psi, d)\n",
    "\n",
    "# 4. Update parameters (your optimization step)\n",
    "F_new = F - learning_rate * process_F_gradients(Fgrads)\n",
    "p_new = p - learning_rate * pgrad\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0n5gqx5mpl6",
   "metadata": {},
   "source": [
    "## Probability Gradient Computation\n",
    "\n",
    "This section implements the gradient computation for the probability vector **p**.\n",
    "\n",
    "### Mathematical Background\n",
    "\n",
    "For the probability $p_i$ of transformation $f_i$, the gradient is:\n",
    "\n",
    "$$\\nabla_{p_i} = \\int_X \\rho_F(x) \\cdot \\psi(f_i(x)) \\, dx$$\n",
    "\n",
    "where:\n",
    "- $\\rho_F(x)$ is the fixed measure\n",
    "- $\\psi$ is the auxiliary potential (scalar field)\n",
    "- $\\psi(f_i(x))$ is the pull-back of $\\psi$ by transformation $f_i$\n",
    "- The integral is a discrete sum over the grid\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "**Key differences from F gradient:**\n",
    "- Uses the **scalar field** ψ (auxiliary potential), not the vector field T\n",
    "- Requires pull-back of a scalar field instead of a vector field\n",
    "- Result is a scalar for each $p_i$, not a vector field\n",
    "\n",
    "**Computation steps:**\n",
    "1. Pull back ψ through each transformation: $\\psi(f_i(x))$\n",
    "2. Multiply by fixed measure: $\\rho_F(x) \\cdot \\psi(f_i(x))$\n",
    "3. Integrate (sum) over the grid: $\\sum_x \\rho_F(x) \\cdot \\psi(f_i(x))$\n",
    "\n",
    "### Separation from F Gradient\n",
    "\n",
    "The gradients w.r.t. F and p are computed **separately** because:\n",
    "- F gradient needs the **vector field** T (gradient of Brenier potential)\n",
    "- p gradient needs the **scalar field** ψ (auxiliary potential)\n",
    "- They will be handled differently in the optimization loop\n",
    "\n",
    "This separation provides flexibility for different optimization strategies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
